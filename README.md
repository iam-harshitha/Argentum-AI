# ğŸ’¼ Argentum-AI: Your Regulated Financial Chat Assistant

**Argentum-AI** is a smart, locally hosted financial chatbot that leverages Retrieval-Augmented Generation (RAG) and cutting-edge LLMs (like Meta LLaMA-3 via Groq) to answer investment-related questions from PDF documents.  
It's built for financial compliance, concise answers, and an intuitive chat-like user interface â€” all without relying on cloud storage or third-party PDFs.

---

## ğŸš€ How It Works 

### ğŸ“„ Step 1: Upload a PDF
Users start by uploading a financial document (such as a mutual fund brochure or prospectus) through a simple drag-and-drop interface.

### ğŸ“ Step 2: Parsing & Chunking
The uploaded PDF is read and split into smaller, semantically meaningful text chunks using **PyMuPDF** and custom logic â€” ensuring high-quality retrieval.

### ğŸ§  Step 3: Vectorization & FAISS Indexing
Each chunk is embedded using **SentenceTransformers** (`all-MiniLM-L6-v2`) and stored in a local **FAISS** vector index. This makes real-time semantic search blazing fast.

### ğŸ” Step 4: Retrieval Based on User Query
When a user types a question, the top relevant document chunks are retrieved from the FAISS index based on vector similarity.

### ğŸ“¾ Step 5: Prompting the LLM
These chunks are combined with a carefully structured prompt and sent to the **Meta LLaMA-3 8B model** hosted via **Groq API**, ensuring responses are grounded in the retrieved context.

### ğŸ’¬ Step 6: Response Display (Chat Interface)
The response is shown in a sleek, conversational interface built using **Streamlit**, where previous questions and answers are retained â€” allowing ongoing Q&A in a natural chat flow.

---

## ğŸ› ï¸ Tech Stack & Purpose

| Technology | Role |
|------------|------|
| **Streamlit** | ğŸ‹ Frontend UI for chat and PDF upload |
| **PyMuPDF** | ğŸ“„ Extract text from PDF files |
| **SentenceTransformers** | ğŸ“Œ Generate dense vector embeddings for document chunks |
| **FAISS** | âš¡ Build and search a fast local vector store |
| **Groq API** | ğŸ§  Call Meta LLaMA-3 for concise, contextual answer generation |
| **Transformers** | ğŸ”€ Handles tokenizer and language model structure |
| **Requests** | ğŸ“¡ Manage external HTTP API communication |
| **Python** | ğŸ Core programming language for backend logic |


---

## ğŸ§° Source Files 

| File | Description |
|------|-------------|
| `app.py` | ğŸ¯ Main Streamlit app: Handles UI, session, prompt construction, and full orchestration |
| `loader.py` | ğŸ“„ Extracts and chunks PDF text |
| `rag.py` | ğŸ§  Builds FAISS index, handles vector search |
| `llm.py` | ğŸ¤– Sends prompts to Groq-hosted Meta LLaMA-3 and returns clean answers |
| `.env` | ğŸ” Stores your Groq API key (`GROQ_API_KEY`) securely |
| `requirements.txt` | ğŸ“† Lists Python dependencies |

---

## ğŸ–¼ï¸ Screenshots
![Screenshot 2025-04-16 122504](https://github.com/user-attachments/assets/737cc6b9-7286-4f5f-a18d-ac9ae48ac231)
![Screenshot 2025-04-16 122520](https://github.com/user-attachments/assets/1ac86e91-9ce6-4dfb-b4df-103b42e71990)


## ğŸ”® Future Improvements

- Support uploading and processing multiple PDFs
- Add logging and history tracking of user queries
- Include optional references to source chunks in responses
- Allow download/export of chat history
- Potential cloud deployment for wider access

---

